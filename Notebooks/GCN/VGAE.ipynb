{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch.nn import Module, Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Graph AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_degree_matrix(A) -> torch.sparse.FloatTensor:\n",
    "    D = scipy.sparse.spdiags(torch.sparse.sum(A, dim=1).to_dense(),0, *A.shape).tocoo()\n",
    "    return coo_to_torch_sparse_matrix(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coo_to_torch_sparse_matrix(coo):\n",
    "    idx = torch.LongTensor(coo.nonzero())\n",
    "    data = torch.FloatTensor(coo.data)\n",
    "    return torch.sparse.FloatTensor(idx, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(A, normalized=True):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    D = sparse_degree_matrix(A)\n",
    "    if not normalized:\n",
    "        L = D.sub(A)\n",
    "    else:\n",
    "        D = (D**(-0.5)).to_dense()\n",
    "        p1 = torch.spmm(A, D)\n",
    "        L = torch.mm(p1, D)\n",
    "        # L = coo_to_torch_sparse_matrix(scipy.sparse.coo_matrix(I - p2))\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_test = torch.sparse.FloatTensor(data.edge_index, torch.FloatTensor(np.repeat(1, len(data.edge_index[1]))))\n",
    "L = laplacian(M_test)\n",
    "# L = torch.from_numpy(np.load('imported_A.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc (adj_rec, adj_label) -> float:\n",
    "    labels_all = adj_label.view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def get_roc_auc_score (adj_rec, adj_label) -> float:\n",
    "    print(\"Rec \", adj_rec[0][0].detach(), \"\\tOrig \", adj_label[0][0], sep='')\n",
    "    labels_all = adj_label.view(-1).long()\n",
    "    preds_all = adj_rec.view(-1).long()\n",
    "    return roc_auc_score(labels_all, preds_all)\n",
    "    \n",
    "def get_ap_score (adj_rec, adj_label) -> float:\n",
    "    labels_all = adj_label.view(-1).long()\n",
    "    preds_all = adj_rec.view(-1).long()\n",
    "    return average_precision_score(labels_all, preds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(input_dim, output_dim) -> Parameter:\n",
    "    init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "    return Parameter(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolutionLayer, self).__init__()\n",
    "        self.weight = glorot_init(in_features, out_features)\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        return output # + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(Module):\n",
    "    def __init__(self, nfeat, nhid, dimz, n_samples):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.nhid = nhid\n",
    "        self.gc1 = GraphConvolutionLayer(nfeat, nhid)\n",
    "        self.gc2mu = GraphConvolutionLayer(nhid, dimz)\n",
    "        self.gc2std = GraphConvolutionLayer(nhid, dimz)\n",
    "        \n",
    "    def encode(self, x, adj):\n",
    "        # First GCN\n",
    "        x = self.gc1(x, adj)\n",
    "        # Mean and std\n",
    "        self.mean = self.gc2mu(x, adj)\n",
    "        self.log_std = self.gc2std(x, adj)\n",
    "        # Latent representation\n",
    "        return torch.exp(self.log_std) * torch.randn(*self.log_std.size()) + self.mean\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        z = self.encode(x, adj)\n",
    "        A_recon = self.decode(z)\n",
    "        return A_recon\n",
    "    \n",
    "    def loss(self, A_hat, A, norm, weight_tensor):\n",
    "        # Binary Crossentropy loss\n",
    "        # bce_loss = nn.BCELoss(reduce='sum')(A_hat, A)\n",
    "        bce_loss = norm * F.binary_cross_entropy(A_hat.view(-1), A.view(-1), weight = weight_tensor)\n",
    "        # KL Loss\n",
    "        # kl_loss = - 0.5 * torch.mean(torch.sum(1 + self.log_std - self.mean.pow(2) - self.log_std.exp(), dim=1)) \n",
    "        # kl_loss = nn.KLDivLoss(reduce='batchmean')(torch.log(A_hat), A)\n",
    "        kl_loss = 0.5/ A_hat.size(0) * (1 + 2*self.log_std - self.mean.pow(2) - torch.exp(self.log_std).pow(2)).sum(1).mean()\n",
    "        \n",
    "        return bce_loss - kl_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae = VGAE(1433, 32, 16, data.x.shape[0])\n",
    "optimizer = optim.Adam(vgae.parameters(), lr=0.01)\n",
    "\n",
    "pos_weight = float(L.shape[0] * L.shape[0] - L.sum()) / L.sum()\n",
    "norm = L.shape[0] * L.shape[0] / float((L.shape[0] * L.shape[0] - L.sum()) * 2)\n",
    "# L += torch.eye(*L.shape)\n",
    "weight_mask = L.view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec tensor(1.)\tOrig tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/200 [00:03<11:43,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.4963)\tROC AUC score: 0.5090135784923551\tAP score: 0.0018764116671314414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [00:19<00:45,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec tensor(0.7228)\tOrig tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 51/200 [00:22<02:40,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.4896)\tROC AUC score: 0.49999986343849845\tAP score: 0.001439468154971647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [00:36<00:32,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec tensor(0.8292)\tOrig tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 101/200 [00:39<01:52,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.4723)\tROC AUC score: 0.49999986343849845\tAP score: 0.001439468154971647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [00:55<00:16,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec tensor(0.9184)\tOrig tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 151/200 [00:58<01:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.4889)\tROC AUC score: 0.4999997268769969\tAP score: 0.001439468154971647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:14<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "loss_history = np.zeros(200) \n",
    "\n",
    "for epoch in tqdm.trange(200): \n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    # A_hat = vgae(torch.eye(*data.x.shape), L)\n",
    "    A_hat = vgae(data.x, L)\n",
    "    loss = vgae.loss(A_hat, L, norm, weight_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0: \n",
    "        print(\"Accuracy: \" + str(get_acc(A_hat, L).detach()) + \n",
    "              \"\\tROC AUC score: \" + str(get_roc_auc_score(A_hat, M_test.to_dense())) +\n",
    "              \"\\tAP score: \" + str(get_ap_score(A_hat, M_test.to_dense()))\n",
    "             )\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1495308876037598\n",
      "0.38592711091041565\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2500, 0.0000]])\n",
      "tensor([[0.9379, 0.5728, 0.6636,  ..., 0.5226, 0.6639, 0.5218],\n",
      "        [0.5728, 0.9919, 0.9693,  ..., 0.0963, 0.4516, 0.4084],\n",
      "        [0.6636, 0.9693, 0.9392,  ..., 0.2121, 0.4718, 0.4385],\n",
      "        ...,\n",
      "        [0.5226, 0.0963, 0.2121,  ..., 0.9855, 0.5712, 0.4438],\n",
      "        [0.6639, 0.4516, 0.4718,  ..., 0.5712, 0.6544, 0.5269],\n",
      "        [0.5218, 0.4084, 0.4385,  ..., 0.4438, 0.5269, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "print(loss_history[0])\n",
    "print(loss_history[len(loss_history) -1])\n",
    "print(L, A_hat.data, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN vs VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, A):\n",
    "    model.eval()\n",
    "    _, pred = model(data.x, A).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Module):\n",
    "    def __init__(self, nfeat, nhid, nclass):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolutionLayer(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolutionLayer(nhid, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGAE_A = A_hat.data.clone()\n",
    "VGAE_A = (VGAE_A > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn1 = GCN(1433, 50, 7)\n",
    "gcn2 = GCN(1433, 50, 7)\n",
    "optimizer_gcn_1 = optim.Adam(gcn1.parameters())\n",
    "optimizer_gcn_2 = optim.Adam(gcn2.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_history, loss\n",
    "\n",
    "import tqdm\n",
    "loss_history = np.zeros(2500) \n",
    "\n",
    "for epoch in tqdm.trange(2500): \n",
    "  \n",
    "    optimizer_gcn_1.zero_grad()\n",
    "    outputs = gcn1(data.x, L) # Usiamo tutto il dataset\n",
    "    loss = criterion(outputs[data.train_mask], data.y[data.train_mask]) # Mascheriamo sulla parte di training\n",
    "    loss.backward()\n",
    "    optimizer_gcn_1.step()\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_history[len(loss_history) -1])\n",
    "test(gcn1, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_history, loss\n",
    "\n",
    "import tqdm\n",
    "loss_history = np.zeros(2500) \n",
    "\n",
    "for epoch in tqdm.trange(2500): \n",
    "  \n",
    "    optimizer_gcn_2.zero_grad()\n",
    "    outputs = gcn2(data.x, VGAE_A) # Usiamo tutto il dataset\n",
    "    loss = criterion(outputs[data.train_mask], data.y[data.train_mask]) # Mascheriamo sulla parte di training\n",
    "    loss.backward()\n",
    "    optimizer_gcn_2.step()\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_history[len(loss_history) -1])\n",
    "test(gcn2, VGAE_A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
