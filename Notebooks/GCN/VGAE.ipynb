{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch.nn import Module, Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Graph AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_degree_matrix(A) -> torch.sparse.FloatTensor:\n",
    "    D = scipy.sparse.spdiags(torch.sparse.sum(A, dim=1).to_dense(),0, *A.shape).tocoo()\n",
    "    return coo_to_torch_sparse_matrix(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coo_to_torch_sparse_matrix(coo):\n",
    "    idx = torch.LongTensor(coo.nonzero())\n",
    "    data = torch.FloatTensor(coo.data)\n",
    "    return torch.sparse.FloatTensor(idx, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(A, normalized=True):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    D = sparse_degree_matrix(A)\n",
    "    if not normalized:\n",
    "        L = D.sub(A)\n",
    "    else:\n",
    "        D = (D**(-0.5)).to_dense()\n",
    "        p1 = torch.spmm(A, D)\n",
    "        L = torch.mm(p1, D)\n",
    "        # L = coo_to_torch_sparse_matrix(scipy.sparse.coo_matrix(I - p2))\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_test = torch.sparse.FloatTensor(data.edge_index, torch.FloatTensor(np.repeat(1, len(data.edge_index[1]))))\n",
    "L = laplacian(M_test)\n",
    "# L = torch.from_numpy(np.load('imported_A.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(input_dim, output_dim):\n",
    "    init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "    return Parameter(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolutionLayer, self).__init__()\n",
    "        self.weight = glorot_init(in_features, out_features)\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        return output # + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(Module):\n",
    "    def __init__(self, nfeat, nhid, dimz, n_samples):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.nhid = nhid\n",
    "        self.gc1 = GraphConvolutionLayer(nfeat, nhid)\n",
    "        self.gc2mu = GraphConvolutionLayer(nhid, dimz)\n",
    "        self.gc2std = GraphConvolutionLayer(nhid, dimz)\n",
    "        \n",
    "    def encode(self, x, adj):\n",
    "        # First GCN\n",
    "        x = self.gc1(x, adj)\n",
    "        # Mean and std\n",
    "        self.mean = self.gc2mu(x, adj)\n",
    "        self.log_std = self.gc2std(x, adj)\n",
    "        # Latent representation\n",
    "        return torch.exp(self.log_std) * torch.randn(*self.log_std.size()) + self.mean\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        z = self.encode(x, adj)\n",
    "        A_recon = self.decode(z)\n",
    "        return A_recon\n",
    "    \n",
    "    def loss(self, A_hat, A, norm, weight_tensor):\n",
    "        # Binary Crossentropy loss\n",
    "        # bce_loss = nn.BCELoss(reduce='sum')(A_hat, A)\n",
    "        bce_loss = norm * F.binary_cross_entropy(A_hat.view(-1), A.view(-1), weight = weight_tensor)\n",
    "        # KL Loss\n",
    "        # kl_loss = - 0.5 * torch.mean(torch.sum(1 + self.log_std - self.mean.pow(2) - self.log_std.exp(), dim=1)) \n",
    "        # kl_loss = nn.KLDivLoss(reduce='batchmean')(torch.log(A_hat), A)\n",
    "        kl_loss = 0.5/ A_hat.size(0) * (1 + 2*self.log_std - self.mean.pow(2) - torch.exp(self.log_std)).sum(1).mean()\n",
    "        \n",
    "        return bce_loss - kl_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae = VGAE(1433, 32, 16, data.x.shape[0])\n",
    "optimizer = optim.Adam(vgae.parameters(), lr=0.01)\n",
    "\n",
    "pos_weight = float(L.shape[0] * L.shape[0] - L.sum()) / L.sum()\n",
    "norm = L.shape[0] * L.shape[0] / float((L.shape[0] * L.shape[0] - L.sum()) * 2)\n",
    "# L += torch.eye(*L.shape)\n",
    "weight_mask = L.view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "loss_history = np.zeros(1000) \n",
    "\n",
    "for epoch in tqdm.trange(1000): \n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    # A_hat = vgae(torch.eye(*data.x.shape), L)\n",
    "    A_hat = vgae(data.x, L)\n",
    "    loss = vgae.loss(A_hat, L, norm, weight_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0: print(\"Accuracy: \" + str(get_acc(A_hat, L).detach()))\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "print(loss_history[0])\n",
    "print(loss_history[len(loss_history) -1])\n",
    "print(L, A_hat.data, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN vs VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, A):\n",
    "    model.eval()\n",
    "    _, pred = model(data.x, A).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Module):\n",
    "    def __init__(self, nfeat, nhid, nclass):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolutionLayer(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolutionLayer(nhid, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGAE_A = A_hat.data.clone()\n",
    "VGAE_A = (VGAE_A > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn1 = GCN(1433, 50, 7)\n",
    "gcn2 = GCN(1433, 50, 7)\n",
    "optimizer_gcn_1 = optim.Adam(gcn1.parameters())\n",
    "optimizer_gcn_2 = optim.Adam(gcn2.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_history, loss\n",
    "\n",
    "import tqdm\n",
    "loss_history = np.zeros(2500) \n",
    "\n",
    "for epoch in tqdm.trange(2500): \n",
    "  \n",
    "    optimizer_gcn_1.zero_grad()\n",
    "    outputs = gcn1(data.x, L) # Usiamo tutto il dataset\n",
    "    loss = criterion(outputs[data.train_mask], data.y[data.train_mask]) # Mascheriamo sulla parte di training\n",
    "    loss.backward()\n",
    "    optimizer_gcn_1.step()\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_history[len(loss_history) -1])\n",
    "test(gcn1, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_history, loss\n",
    "\n",
    "import tqdm\n",
    "loss_history = np.zeros(2500) \n",
    "\n",
    "for epoch in tqdm.trange(2500): \n",
    "  \n",
    "    optimizer_gcn_2.zero_grad()\n",
    "    outputs = gcn2(data.x, VGAE_A) # Usiamo tutto il dataset\n",
    "    loss = criterion(outputs[data.train_mask], data.y[data.train_mask]) # Mascheriamo sulla parte di training\n",
    "    loss.backward()\n",
    "    optimizer_gcn_2.step()\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_history[len(loss_history) -1])\n",
    "test(gcn2, VGAE_A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
