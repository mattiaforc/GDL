{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Neural Networks\n",
    "## Graph LeNet5 with PyTorch\n",
    "### Xavier Bresson, Oct. 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on ArXiv preprint: [arXiv:1606.09375](https://arxiv.org/pdf/1606.09375.pdf) <br>\n",
    "and [this pytorch implementation](https://github.com/xbresson/spectral_graph_convnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pdb #pdb.set_trace()\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'lib/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(1)\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "    torch.manual_seed(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "(5000, 784)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "val_data = np.reshape(train_data[55000:], (train_data[55000:].shape[0], -1))/255.0\n",
    "val_labels = train_labels[55000:]\n",
    "train_data = np.reshape(train_data[0:55000], (train_data[0:55000].shape[0], -1))/255.0\n",
    "train_labels = train_labels[0:55000]\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], -1))/255.0\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb edges:  6396\n",
      "Heavy Edge Matching coarsening with Xavier version\n",
      "Layer 0: M_0 = |V| = 976 nodes (192 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 488 nodes (83 added), |E| = 1619 edges\n",
      "Layer 2: M_2 = |V| = 244 nodes (29 added), |E| = 794 edges\n",
      "Layer 3: M_3 = |V| = 122 nodes (7 added), |E| = 396 edges\n",
      "Layer 4: M_4 = |V| = 61 nodes (0 added), |E| = 194 edges\n",
      "lmax: [1.3857538, 1.3440963, 1.1994357, 1.0239158]\n",
      "(55000, 976)\n",
      "(5000, 976)\n",
      "(10000, 976)\n",
      "Execution time: 1.81s\n"
     ]
    }
   ],
   "source": [
    "from lib.grid_graph import grid_graph\n",
    "from lib.coarsening import coarsen\n",
    "from lib.coarsening import lmax_L\n",
    "\n",
    "from lib.coarsening import perm_data\n",
    "from lib.coarsening import rescale_L\n",
    "\n",
    "# Construct graph\n",
    "t_start = time.time()\n",
    "grid_side = 28\n",
    "number_edges = 8\n",
    "metric = 'euclidean'\n",
    "A = grid_graph(grid_side,number_edges,metric) # create graph of Euclidean grid\n",
    "\n",
    "# Compute coarsened graphs\n",
    "coarsening_levels = 4\n",
    "L, perm = coarsen(A, coarsening_levels)\n",
    "\n",
    "# Compute max eigenvalue of graph Laplacians\n",
    "lmax = []\n",
    "for i in range(coarsening_levels):\n",
    "    lmax.append(lmax_L(L[i]))\n",
    "print('lmax: ' + str([lmax[i] for i in range(coarsening_levels)]))\n",
    "\n",
    "# Reindex nodes to satisfy a binary tree structure\n",
    "train_data = perm_data(train_data, perm)\n",
    "val_data = perm_data(val_data, perm)\n",
    "test_data = perm_data(test_data, perm)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "del perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph ConvNet LeNet5\n",
    "### Layers: CL32-MP4-CL64-MP4-FC512-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class definitions\n",
    "\n",
    "class my_sparse_mm(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Implementation of a new autograd function for sparse variables, \n",
    "    called \"my_sparse_mm\", by subclassing torch.autograd.Function \n",
    "    and implementing the forward and backward passes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, W, x):  # W is SPARSE\n",
    "        self.save_for_backward(W, x)\n",
    "        y = torch.mm(W, x)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        W, x = self.saved_tensors \n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input_dL_dW = torch.mm(grad_input, x.t()) \n",
    "        grad_input_dL_dx = torch.mm(W.t(), grad_input )\n",
    "        return grad_input_dL_dW, grad_input_dL_dx\n",
    "    \n",
    "    \n",
    "class Graph_ConvNet_LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        \n",
    "        print('Graph ConvNet: LeNet5')\n",
    "        \n",
    "        super(Graph_ConvNet_LeNet5, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
    "        FC1Fin = CL2_F*(D//16)\n",
    "        \n",
    "        # graph CL1\n",
    "        self.cl1 = nn.Linear(CL1_K, CL1_F) \n",
    "        Fin = CL1_K; Fout = CL1_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.cl1.weight.data.uniform_(-scale, scale)\n",
    "        self.cl1.bias.data.fill_(0.0)\n",
    "        self.CL1_K = CL1_K; self.CL1_F = CL1_F; \n",
    "        \n",
    "        # graph CL2\n",
    "        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F) \n",
    "        Fin = CL2_K*CL1_F; Fout = CL2_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.cl2.weight.data.uniform_(-scale, scale)\n",
    "        self.cl2.bias.data.fill_(0.0)\n",
    "        self.CL2_K = CL2_K; self.CL2_F = CL2_F; \n",
    "\n",
    "        # FC1\n",
    "        self.fc1 = nn.Linear(FC1Fin, FC1_F) \n",
    "        Fin = FC1Fin; Fout = FC1_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.fc1.weight.data.uniform_(-scale, scale)\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "        self.FC1Fin = FC1Fin\n",
    "        \n",
    "        # FC2\n",
    "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
    "        Fin = FC1_F; Fout = FC2_F;\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        self.fc2.weight.data.uniform_(-scale, scale)\n",
    "        self.fc2.bias.data.fill_(0.0)\n",
    "\n",
    "        # nb of parameters\n",
    "        nb_param = CL1_K* CL1_F + CL1_F          # CL1\n",
    "        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n",
    "        nb_param += FC1Fin* FC1_F + FC1_F        # FC1\n",
    "        nb_param += FC1_F* FC2_F + FC2_F         # FC2\n",
    "        print('nb of parameters=',nb_param,'\\n')\n",
    "        \n",
    "        \n",
    "    def init_weights(self, W, Fin, Fout):\n",
    "\n",
    "        scale = np.sqrt( 2.0/ (Fin+Fout) )\n",
    "        W.uniform_(-scale, scale)\n",
    "\n",
    "        return W\n",
    "        \n",
    "        \n",
    "    def graph_conv_cheby(self, x, cl, L, lmax, Fout, K):\n",
    "\n",
    "        # parameters\n",
    "        # B = batch size\n",
    "        # V = nb vertices\n",
    "        # Fin = nb input features\n",
    "        # Fout = nb output features\n",
    "        # K = Chebyshev order & support size\n",
    "        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin) \n",
    "\n",
    "        # rescale Laplacian\n",
    "        lmax = lmax_L(L)\n",
    "        L = rescale_L(L, lmax) \n",
    "        \n",
    "        # convert scipy sparse matric L to pytorch\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col)).T \n",
    "        indices = indices.astype(np.int64)\n",
    "        indices = torch.from_numpy(indices)\n",
    "        indices = indices.type(torch.LongTensor)\n",
    "        L_data = L.data.astype(np.float32)\n",
    "        L_data = torch.from_numpy(L_data) \n",
    "        L_data = L_data.type(torch.FloatTensor)\n",
    "        L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))\n",
    "        L = Variable( L , requires_grad=False)\n",
    "        if torch.cuda.is_available():\n",
    "            L = L.cuda()\n",
    "        \n",
    "        # transform to Chebyshev basis\n",
    "        x0 = x.permute(1,2,0).contiguous()  # V x Fin x B\n",
    "        x0 = x0.view([V, Fin*B])            # V x Fin*B\n",
    "        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n",
    "        \n",
    "        def concat(x, x_):\n",
    "            x_ = x_.unsqueeze(0)            # 1 x V x Fin*B\n",
    "            return torch.cat((x, x_), 0)    # K x V x Fin*B  \n",
    "             \n",
    "        if K > 1: \n",
    "            x1 = torch.spmm(L, x0)              # V x Fin*B\n",
    "            # x1 = my_sparse_mm()(L, x0)\n",
    "            x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * torch.spmm(L,x1) - x0  \n",
    "            # x2 = 2 * my_sparse_mm()(L, x1) -x0\n",
    "            x = torch.cat((x, x2.unsqueeze(0)),0)  # M x Fin*B\n",
    "            x0, x1 = x1, x2  \n",
    "        \n",
    "        x = x.view([K, V, Fin, B])           # K x V x Fin x B     \n",
    "        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K       \n",
    "        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n",
    "        \n",
    "        # Compose linearly Fin features to get Fout features\n",
    "        x = cl(x)                            # B*V x Fout  \n",
    "        x = x.view([B, V, Fout])             # B x V x Fout\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    # Max pooling of size p. Must be a power of 2.\n",
    "    def graph_max_pool(self, x, p): \n",
    "        if p > 1: \n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p          \n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            return x  \n",
    "        else:\n",
    "            return x    \n",
    "        \n",
    "        \n",
    "    def forward(self, x, d, L, lmax):\n",
    "        \n",
    "        # graph CL1\n",
    "        x = x.unsqueeze(2) # B x V x Fin=1  \n",
    "        x = self.graph_conv_cheby(x, self.cl1, L[0], lmax[0], self.CL1_F, self.CL1_K)\n",
    "        x = F.relu(x)\n",
    "        x = self.graph_max_pool(x, 4)\n",
    "        \n",
    "        # graph CL2\n",
    "        x = self.graph_conv_cheby(x, self.cl2, L[2], lmax[2], self.CL2_F, self.CL2_K)\n",
    "        x = F.relu(x)\n",
    "        x = self.graph_max_pool(x, 4)\n",
    "        \n",
    "        # FC1\n",
    "        x = x.view(-1, self.FC1Fin)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x  = nn.Dropout(d)(x)\n",
    "        \n",
    "        # FC2\n",
    "        x = self.fc2(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def loss(self, y, y_target, l2_regularization):\n",
    "    \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            data = param* param\n",
    "            l2_loss += data.sum()\n",
    "           \n",
    "        loss += 0.5* l2_regularization* l2_loss\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def update(self, lr):\n",
    "                \n",
    "        update = torch.optim.SGD( self.parameters(), lr=lr, momentum=0.9 )\n",
    "        \n",
    "        return update\n",
    "        \n",
    "        \n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "   \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    def evaluation(self, y_predicted, test_l):\n",
    "    \n",
    "        _, class_predicted = torch.max(y_predicted.data, 1)\n",
    "        return 100.0* (class_predicted == test_l).sum()/ y_predicted.size(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete existing network\n",
      "\n",
      "Graph ConvNet: LeNet5\n",
      "nb of parameters= 2056586 \n",
      "\n",
      "Graph_ConvNet_LeNet5(\n",
      "  (cl1): Linear(in_features=25, out_features=32, bias=True)\n",
      "  (cl2): Linear(in_features=800, out_features=64, bias=True)\n",
      "  (fc1): Linear(in_features=3904, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "num_epochs= 20 , train_size= 55000 , nb_iter= 11000\n",
      "epoch= 1, i=  100, loss(batch)= 0.3325, accuray(batch)= 93.00\n",
      "epoch= 1, i=  200, loss(batch)= 0.2079, accuray(batch)= 99.00\n",
      "epoch= 1, i=  300, loss(batch)= 0.2897, accuray(batch)= 95.00\n",
      "epoch= 1, i=  400, loss(batch)= 0.1583, accuray(batch)= 99.00\n",
      "epoch= 1, i=  500, loss(batch)= 0.1964, accuray(batch)= 96.00\n",
      "epoch= 1, loss(train)= 0.385, accuracy(train)= 91.109, time= 209.650, lr= 0.05000\n",
      "  accuracy(test) = 97.730 %, time= 23.007\n",
      "epoch= 2, i=  100, loss(batch)= 0.2038, accuray(batch)= 96.00\n",
      "epoch= 2, i=  200, loss(batch)= 0.1846, accuray(batch)= 96.00\n",
      "epoch= 2, i=  300, loss(batch)= 0.1778, accuray(batch)= 98.00\n",
      "epoch= 2, i=  400, loss(batch)= 0.2142, accuray(batch)= 96.00\n",
      "epoch= 2, i=  500, loss(batch)= 0.2337, accuray(batch)= 95.00\n",
      "epoch= 2, loss(train)= 0.182, accuracy(train)= 97.687, time= 207.808, lr= 0.04750\n",
      "  accuracy(test) = 98.460 %, time= 23.073\n",
      "epoch= 3, i=  100, loss(batch)= 0.1319, accuray(batch)= 100.00\n",
      "epoch= 3, i=  200, loss(batch)= 0.1300, accuray(batch)= 99.00\n",
      "epoch= 3, i=  300, loss(batch)= 0.2237, accuray(batch)= 96.00\n",
      "epoch= 3, i=  400, loss(batch)= 0.1130, accuray(batch)= 99.00\n",
      "epoch= 3, i=  500, loss(batch)= 0.1927, accuray(batch)= 98.00\n",
      "epoch= 3, loss(train)= 0.150, accuracy(train)= 98.369, time= 207.866, lr= 0.04512\n",
      "  accuracy(test) = 98.650 %, time= 23.141\n",
      "epoch= 4, i=  100, loss(batch)= 0.1414, accuray(batch)= 98.00\n",
      "epoch= 4, i=  200, loss(batch)= 0.2746, accuray(batch)= 95.00\n",
      "epoch= 4, i=  300, loss(batch)= 0.1208, accuray(batch)= 100.00\n",
      "epoch= 4, i=  400, loss(batch)= 0.1412, accuray(batch)= 97.00\n",
      "epoch= 4, i=  500, loss(batch)= 0.1370, accuray(batch)= 98.00\n",
      "epoch= 4, loss(train)= 0.132, accuracy(train)= 98.647, time= 207.800, lr= 0.04287\n",
      "  accuracy(test) = 98.840 %, time= 22.984\n",
      "epoch= 5, i=  100, loss(batch)= 0.1035, accuray(batch)= 99.00\n",
      "epoch= 5, i=  200, loss(batch)= 0.1308, accuray(batch)= 99.00\n",
      "epoch= 5, i=  300, loss(batch)= 0.1852, accuray(batch)= 98.00\n",
      "epoch= 5, i=  400, loss(batch)= 0.1060, accuray(batch)= 99.00\n",
      "epoch= 5, i=  500, loss(batch)= 0.1100, accuray(batch)= 98.00\n",
      "epoch= 5, loss(train)= 0.118, accuracy(train)= 98.860, time= 208.124, lr= 0.04073\n",
      "  accuracy(test) = 98.960 %, time= 22.942\n",
      "epoch= 6, i=  100, loss(batch)= 0.1207, accuray(batch)= 98.00\n",
      "epoch= 6, i=  200, loss(batch)= 0.0866, accuray(batch)= 100.00\n",
      "epoch= 6, i=  300, loss(batch)= 0.0868, accuray(batch)= 99.00\n",
      "epoch= 6, i=  400, loss(batch)= 0.1333, accuray(batch)= 99.00\n",
      "epoch= 6, i=  500, loss(batch)= 0.1034, accuray(batch)= 99.00\n",
      "epoch= 6, loss(train)= 0.106, accuracy(train)= 99.069, time= 207.924, lr= 0.03869\n",
      "  accuracy(test) = 99.000 %, time= 22.915\n",
      "epoch= 7, i=  100, loss(batch)= 0.0843, accuray(batch)= 100.00\n",
      "epoch= 7, i=  200, loss(batch)= 0.1379, accuray(batch)= 98.00\n",
      "epoch= 7, i=  300, loss(batch)= 0.0751, accuray(batch)= 100.00\n",
      "epoch= 7, i=  400, loss(batch)= 0.1086, accuray(batch)= 99.00\n",
      "epoch= 7, i=  500, loss(batch)= 0.0928, accuray(batch)= 99.00\n",
      "epoch= 7, loss(train)= 0.099, accuracy(train)= 99.116, time= 207.744, lr= 0.03675\n",
      "  accuracy(test) = 98.970 %, time= 22.990\n",
      "epoch= 8, i=  100, loss(batch)= 0.1138, accuray(batch)= 98.00\n",
      "epoch= 8, i=  200, loss(batch)= 0.1183, accuray(batch)= 98.00\n",
      "epoch= 8, i=  300, loss(batch)= 0.0746, accuray(batch)= 100.00\n",
      "epoch= 8, i=  400, loss(batch)= 0.0858, accuray(batch)= 99.00\n",
      "epoch= 8, i=  500, loss(batch)= 0.1170, accuray(batch)= 99.00\n",
      "epoch= 8, loss(train)= 0.094, accuracy(train)= 99.178, time= 208.077, lr= 0.03492\n",
      "  accuracy(test) = 99.000 %, time= 22.950\n",
      "epoch= 9, i=  100, loss(batch)= 0.0732, accuray(batch)= 100.00\n",
      "epoch= 9, i=  200, loss(batch)= 0.0799, accuray(batch)= 99.00\n",
      "epoch= 9, i=  300, loss(batch)= 0.0824, accuray(batch)= 99.00\n",
      "epoch= 9, i=  400, loss(batch)= 0.0684, accuray(batch)= 100.00\n",
      "epoch= 9, i=  500, loss(batch)= 0.0714, accuray(batch)= 100.00\n",
      "epoch= 9, loss(train)= 0.088, accuracy(train)= 99.224, time= 207.927, lr= 0.03317\n",
      "  accuracy(test) = 99.160 %, time= 22.954\n",
      "epoch= 10, i=  100, loss(batch)= 0.0635, accuray(batch)= 100.00\n",
      "epoch= 10, i=  200, loss(batch)= 0.0839, accuray(batch)= 99.00\n",
      "epoch= 10, i=  300, loss(batch)= 0.0744, accuray(batch)= 100.00\n",
      "epoch= 10, i=  400, loss(batch)= 0.0760, accuray(batch)= 100.00\n",
      "epoch= 10, i=  500, loss(batch)= 0.0783, accuray(batch)= 100.00\n",
      "epoch= 10, loss(train)= 0.082, accuracy(train)= 99.315, time= 207.820, lr= 0.03151\n",
      "  accuracy(test) = 98.990 %, time= 22.967\n",
      "epoch= 11, i=  100, loss(batch)= 0.0897, accuray(batch)= 98.00\n",
      "epoch= 11, i=  200, loss(batch)= 0.0609, accuray(batch)= 100.00\n",
      "epoch= 11, i=  300, loss(batch)= 0.0634, accuray(batch)= 100.00\n",
      "epoch= 11, i=  400, loss(batch)= 0.0605, accuray(batch)= 100.00\n",
      "epoch= 11, i=  500, loss(batch)= 0.1005, accuray(batch)= 98.00\n",
      "epoch= 11, loss(train)= 0.077, accuracy(train)= 99.416, time= 207.722, lr= 0.02994\n",
      "  accuracy(test) = 99.020 %, time= 23.108\n",
      "epoch= 12, i=  100, loss(batch)= 0.0589, accuray(batch)= 100.00\n",
      "epoch= 12, i=  200, loss(batch)= 0.1290, accuray(batch)= 98.00\n",
      "epoch= 12, i=  300, loss(batch)= 0.0677, accuray(batch)= 100.00\n",
      "epoch= 12, i=  400, loss(batch)= 0.0678, accuray(batch)= 100.00\n",
      "epoch= 12, i=  500, loss(batch)= 0.0565, accuray(batch)= 100.00\n",
      "epoch= 12, loss(train)= 0.073, accuracy(train)= 99.480, time= 207.757, lr= 0.02844\n",
      "  accuracy(test) = 99.010 %, time= 22.967\n",
      "epoch= 13, i=  100, loss(batch)= 0.0588, accuray(batch)= 100.00\n",
      "epoch= 13, i=  200, loss(batch)= 0.0659, accuray(batch)= 100.00\n",
      "epoch= 13, i=  300, loss(batch)= 0.0614, accuray(batch)= 100.00\n",
      "epoch= 13, i=  400, loss(batch)= 0.1003, accuray(batch)= 97.00\n",
      "epoch= 13, i=  500, loss(batch)= 0.0708, accuray(batch)= 99.00\n",
      "epoch= 13, loss(train)= 0.070, accuracy(train)= 99.502, time= 210.599, lr= 0.02702\n",
      "  accuracy(test) = 99.040 %, time= 24.267\n"
     ]
    }
   ],
   "source": [
    "# Delete existing network if exists\n",
    "try:\n",
    "    del net\n",
    "    print('Delete existing network\\n')\n",
    "except NameError:\n",
    "    print('No existing network to delete\\n')\n",
    "\n",
    "\n",
    "\n",
    "# network parameters\n",
    "D = train_data.shape[1]\n",
    "CL1_F = 32\n",
    "CL1_K = 25\n",
    "CL2_F = 64\n",
    "CL2_K = 25\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "\n",
    "\n",
    "# instantiate the object net of the class \n",
    "net = Graph_ConvNet_LeNet5(net_parameters)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "print(net)\n",
    "\n",
    "\n",
    "# Weights\n",
    "L_net = list(net.parameters())\n",
    "\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.05\n",
    "dropout_value = 0.5\n",
    "l2_regularization = 5e-4 \n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "global_lr = learning_rate\n",
    "global_step = 0\n",
    "decay = 0.95\n",
    "decay_steps = train_size\n",
    "lr = learning_rate\n",
    "optimizer = net.update(lr) \n",
    "\n",
    "\n",
    "# loop over epochs\n",
    "indices = collections.deque()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # reshuffle \n",
    "    indices.extend(np.random.permutation(train_size)) # rand permutation\n",
    "    \n",
    "    # reset time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # extract batches\n",
    "    running_loss = 0.0\n",
    "    running_accuray = 0\n",
    "    running_total = 0\n",
    "    while len(indices) >= batch_size:\n",
    "        \n",
    "        # extract batches\n",
    "        batch_idx = [indices.popleft() for i in range(batch_size)]\n",
    "        train_x, train_y = train_data[batch_idx,:], train_labels[batch_idx]\n",
    "        train_x = Variable( torch.FloatTensor(train_x).type(dtypeFloat) , requires_grad=False) \n",
    "        train_y = train_y.astype(np.int64)\n",
    "        train_y = torch.LongTensor(train_y).type(dtypeLong)\n",
    "        train_y = Variable( train_y , requires_grad=False) \n",
    "            \n",
    "        # Forward \n",
    "        y = net.forward(train_x, dropout_value, L, lmax)\n",
    "        loss = net.loss(y,train_y,l2_regularization) \n",
    "        loss_train = loss.item()\n",
    "        \n",
    "        # Accuracy\n",
    "        acc_train = net.evaluation(y,train_y.data)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update \n",
    "        global_step += batch_size # to update learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss, accuracy\n",
    "        running_loss += loss_train\n",
    "        running_accuray += acc_train\n",
    "        running_total += 1\n",
    "        \n",
    "        # print        \n",
    "        if not running_total%100: # print every x mini-batches\n",
    "            print('epoch= %d, i= %4d, loss(batch)= %.4f, accuray(batch)= %.2f' % (epoch+1, running_total, loss_train, acc_train))\n",
    "          \n",
    "       \n",
    "    # print \n",
    "    t_stop = time.time() - t_start\n",
    "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
    "          (epoch+1, running_loss/running_total, running_accuray/running_total, t_stop, lr))\n",
    " \n",
    "\n",
    "    # update learning rate \n",
    "    lr = global_lr * pow( decay , float(global_step// decay_steps) )\n",
    "    optimizer = net.update_learning_rate(optimizer, lr)\n",
    "    \n",
    "    \n",
    "    # Test set\n",
    "    running_accuray_test = 0\n",
    "    running_total_test = 0\n",
    "    indices_test = collections.deque()\n",
    "    indices_test.extend(range(test_data.shape[0]))\n",
    "    t_start_test = time.time()\n",
    "    while len(indices_test) >= batch_size:\n",
    "        batch_idx_test = [indices_test.popleft() for i in range(batch_size)]\n",
    "        test_x, test_y = test_data[batch_idx_test,:], test_labels[batch_idx_test]\n",
    "        test_x = Variable( torch.FloatTensor(test_x).type(dtypeFloat) , requires_grad=False) \n",
    "        y = net.forward(test_x, 0.0, L, lmax) \n",
    "        test_y = test_y.astype(np.int64)\n",
    "        test_y = torch.LongTensor(test_y).type(dtypeLong)\n",
    "        test_y = Variable( test_y , requires_grad=False) \n",
    "        acc_test = net.evaluation(y,test_y.data)\n",
    "        running_accuray_test += acc_test\n",
    "        running_total_test += 1\n",
    "    t_stop_test = time.time() - t_start_test\n",
    "    print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}