{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import math\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import ChebConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import get_laplacian\n",
    "data = dataset[0]\n",
    "# L = get_laplacian(data.edge_index, normalization=\"sym\")\n",
    "# L = torch.sparse.FloatTensor(L[0], L[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Sizes of variables</b>: </center>\n",
    "$$L \\in M^{2708\\times 2708}$$\n",
    "$$Features \\in R^{2708\\times 1433}$$\n",
    "$$Labels \\in N^{2708}$$\n",
    "<center>Number of classes: 7</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheb-Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_degree_matrix(A) -> torch.sparse.FloatTensor:\n",
    "    D = scipy.sparse.spdiags(torch.sparse.sum(A, dim=1).to_dense(),0, *A.shape).tocoo()\n",
    "    return coo_to_torch_sparse_matrix(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coo_to_torch_sparse_matrix(coo):\n",
    "    idx = torch.LongTensor(coo.nonzero())\n",
    "    data = torch.FloatTensor(coo.data)\n",
    "    return torch.sparse.FloatTensor(idx, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(A, normalized=True):\n",
    "    I = torch.eye(A.shape[0])\n",
    "    D = sparse_degree_matrix(A)\n",
    "    if not normalized:\n",
    "        L = D.sub(A)\n",
    "    else:\n",
    "        D = (D**(-0.5)).to_dense()\n",
    "        p1 = torch.spmm(A, D)\n",
    "        p2 = torch.mm(p1, D)\n",
    "        L = coo_to_torch_sparse_matrix(scipy.sparse.coo_matrix(I - p2))\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_test = torch.sparse.FloatTensor(data.edge_index, torch.FloatTensor(np.repeat(1, len(data.edge_index[1]))))\n",
    "L = laplacian(M_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    _, pred = model(data.x, M_test).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Convolution_Chebychev(nn.Module):\n",
    "        \n",
    "        def __init__(self, F_in, F_out, K):\n",
    "            super(Graph_Convolution_Chebychev, self).__init__()\n",
    "            self.F_in = F_in\n",
    "            self.F_out = F_out\n",
    "            self.weight = Parameter(torch.FloatTensor(K, F_in, F_out))\n",
    "            self.bias = Parameter(torch.FloatTensor(F_out))\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "            self.weight.data.uniform_(-stdv, stdv)\n",
    "            self.bias.data.fill_(0.0)\n",
    "        \n",
    "        def forward(self, x, L):\n",
    "            Tx_0 = x\n",
    "            out = torch.mm(Tx_0, self.weight[0])\n",
    "\n",
    "            if self.weight.size(0) > 1:\n",
    "                Tx_1 = torch.spmm(L, x)\n",
    "                out = out + torch.matmul(Tx_1, self.weight[1])\n",
    "\n",
    "            for k in range(2, self.weight.size(0)):\n",
    "                Tx_2 = 2 * torch.spmm(L, Tx_1) - Tx_0\n",
    "                out = out + torch.matmul(Tx_2, self.weight[k])\n",
    "                Tx_0, Tx_1 = Tx_1, Tx_2\n",
    "\n",
    "            if self.bias is not None:\n",
    "                out = out + self.bias\n",
    "\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chebychev_convolutional_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(Chebychev_convolutional_network, self).__init__()\n",
    "\n",
    "        self.F_in, self.F_out, self.K, self.dropout = net_parameters\n",
    "        self.GCN1 = Graph_Convolution_Chebychev(self.F_in, 20, K)\n",
    "        self.GCN2 = Graph_Convolution_Chebychev(20, 7, K)\n",
    "        \n",
    "        self.FC1 = nn.Linear(50, 128)\n",
    "        scale = 1. / math.sqrt(self.FC1.weight.size(1))\n",
    "        self.FC1.weight.data.uniform_(-scale, scale)\n",
    "        self.FC1.bias.data.fill_(0.0)\n",
    "        self.FC2 = nn.Linear(128, 7)\n",
    "        scale = 1. / math.sqrt(self.FC2.weight.size(1))\n",
    "        self.FC2.weight.data.uniform_(-scale, scale)\n",
    "        self.FC2.bias.data.fill_(0.0)\n",
    "        \n",
    "    def forward(self, x, L):\n",
    "        x = self.GCN1(x, L)\n",
    "        x = F.relu(x)\n",
    "        # x = self.graph_max_pool(x, 4)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.GCN2(x, L)\n",
    "        # x = self.graph_max_pool(x, 4)\n",
    "        # x = x.view(-1, self.FC1Fin)\n",
    "        # x = self.FC1(x)\n",
    "        # x = F.relu(x)\n",
    "        # x  = F.dropout(x, self.dropout, training=self.training)\n",
    "        # x = self.FC2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)  \n",
    "    \n",
    "    def loss(self, y, y_target, l2_regularization):\n",
    "    \n",
    "        loss = nn.CrossEntropyLoss()(y,y_target)\n",
    "\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            data = param* param\n",
    "            l2_loss += data.sum()\n",
    "           \n",
    "        loss += 0.5* l2_regularization* l2_loss\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_in = 1433\n",
    "F_out = 7\n",
    "K = 2\n",
    "dropout = 0.3\n",
    "net_parameters = [F_in, F_out, K, dropout]\n",
    "\n",
    "cheb_net = Chebychev_convolutional_network(net_parameters)\n",
    "optimizer_cheb = optim.Adam(cheb_net.parameters())\n",
    "test(cheb_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.zeros(1000) \n",
    "\n",
    "for epoch in tqdm.trange(1000): \n",
    "  \n",
    "    outputs = cheb_net(data.x, L) # Usiamo tutto il dataset\n",
    "    loss = cheb_net.loss(outputs[data.train_mask], data.y[data.train_mask], 5e-4) # Mascheriamo sulla parte di training\n",
    "    loss.backward()\n",
    "    optimizer_cheb.step()\n",
    "    optimizer_cheb.zero_grad()\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "test(cheb_net)\n",
    "print(loss)\n",
    "print(loss_history[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pytorch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / data.test_mask.sum().item()\n",
    "    print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebNet_pytorch_geometric(torch.nn.Module):\n",
    "    def __init__(self, dataset, K):\n",
    "        super(ChebNet_pytorch_geometric, self).__init__()\n",
    "        self.conv1 = ChebConv(dataset.num_features, 20, K)\n",
    "        self.conv2 = ChebConv(20, 7, K)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheb_net = ChebNet_pytorch_geometric(data, 2)\n",
    "optimizer = optim.Adam(cheb_net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test(cheb_net)\n",
    "\n",
    "loss_history = np.zeros(1000) \n",
    "\n",
    "for epoch in tqdm.trange(1000): \n",
    "  \n",
    "    outputs = cheb_net(data) # Usiamo tutto il dataset\n",
    "    loss = criterion(outputs[data.train_mask], data.y[data.train_mask]) # Mascheriamo sulla parte di training\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_history[epoch] = loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()\n",
    "test(cheb_net)\n",
    "print(loss)\n",
    "print(loss_history[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
